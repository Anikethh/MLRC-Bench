The paper "Stable Signature is Unstable: Removing Image Watermark from Diffusion Models" proposes a model-targeted attack to remove the Stable Signature watermark from open-source diffusion models . This attack works by fine-tuning the decoder of the diffusion model using a set of non-watermarked images, referred to as the attacking dataset . The method consists of two main steps :   

Step I: Estimate the Denoised Latent Vector

This step aims to estimate a denoised latent vector for each non-watermarked image in the attacking dataset . The approach varies depending on whether the attacker has access to the diffusion model's encoder .   

Encoder-aware Scenario: If the attacker has access to the encoder, they can directly encode a non-watermarked image into the latent space. This encoded latent vector is then used as the estimated denoised latent vector .   

Encoder-agnostic Scenario: If the encoder is not accessible, the attacker needs to estimate the denoised latent vector using only the denoising layers and the watermarked decoder . The paper proposes an optimization method for this. Initially, a latent vector is randomly generated. Then, an iterative process using gradient descent is employed to adjust this latent vector. The goal is to minimize the difference between the output of the watermarked decoder (when given this latent vector as input) and the corresponding non-watermarked image from the attacking dataset . This optimization typically involves minimizing a loss function that considers both pixel-level differences and perceptual similarity .   

Step II: Fine-tune the Decoder

Once the denoised latent vectors are estimated for all the non-watermarked images in the attacking dataset, the second step involves fine-tuning the decoder of the watermarked diffusion model . The objective is to train the decoder so that when it receives the estimated denoised latent vectors as input, it generates images that are visually similar to the non-watermarked images in the attacking dataset . This fine-tuning process typically involves minimizing a loss function that quantifies the difference between the decoder's output and the target non-watermarked image. This loss function often includes a combination of Mean Squared Error (MSE) loss and perceptual loss to ensure both pixel-level accuracy and high visual quality of the generated images . In some cases, an adversarial loss might also be used to further enhance the realism of the generated images .   

The key idea behind this attack is to make the watermarked decoder learn to produce non-watermarked images when given the latent representations of non-watermarked images . The research demonstrates that this fine-tuning approach can effectively remove the Stable Signature watermark while preserving the visual quality of the generated images, suggesting that Stable Signature might not be as robust against such model-targeted attacks as initially claimed . This attack assumes that the attacker has the ability to modify the parameters of the open-source diffusion model's decoder .   

References:
Hu, Y., Jiang, Z., Guo, M., & Gong, N. (2024). Stable signature is unstable: removing image watermark from diffusion models. arXiv preprint arXiv:2405.07145.
